{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Challenge - NLP - 5 - Convolutional Sequence to Sequence Learning\n",
        "## Steps of problem solving\n",
        "First step was preparing an existing solution.\n",
        "1. Model analysis. I read \"Convolutional Sequence to Sequence Learning\" paper. I already worked with it when I was studied NLP in Samsung AI Academy. I checked code correctness of convolutional layers, seq-to-seq model.\n",
        "2. Filling the gaps. I added tokenizer and dataset iterator.\n",
        "3. Bug fixes like updating versions of libraries and other little mistakes.\n",
        "Second step was model optimization.\n",
        "Model had inappropriate parameters and terrible perplexity.\n",
        "I used the parameters of source paper excepting optimizer.\n",
        "\n",
        "## Model parameters\n",
        "After analysis of paper I got few sets of optimal model parameters and started experiments.\n",
        "I worked with 10,15,20 layers, 256,512 hidden units, kernel sizes 3/3, 3/5, Nesterov accelerated gradient (parameter set from paper \"model optimization\" momentum 0.99, learning rate 0.25) and Adam optimizator. Batch sizes 48,64,128. I tested not all combinations of these parameters, but used combinations from paper and tried to improve them using individual parameters from other hyperparameters sets to find best solution.\n",
        "\n",
        "## Best found parameters (9.933 PPL)\n",
        "\n",
        "Best result shows this parameters set from paper:\n",
        "*Our encoder has 15 layers and the decoder has 15 layers, both with 512 hidden\n",
        "units in the first ten layers and 768 units in the subsequent\n",
        "three layers, all using kernel width 3.*\n",
        "\n",
        "In combination with these batch size:\n",
        "*Unless otherwise stated, we use mini-batches of 64 sentences.*\n",
        "\n",
        "Despite the fact that the paper uses the Nesterov accelerated gradient (SGD) optimizer in my final solution I use Adam optimizer.\n",
        "\n",
        "## Comparison of Adam and Nesterov optimizer on final parameters sets\n",
        "\n",
        "Adam validation of perplex on 1 epoch = 25.025\n",
        "Test perplex on 5, 10, 15 epoch = 10.513, 9.279, 9.087\n",
        "After 15 epoch we need to stop, because then we can see growing of validation PPL from 3.975 -> 5.064 -> 10.210 -> 5.957 -> 65141 -> ~num*10^174 (loss explosion)\n",
        "\n",
        "But using Nesterov optimizer we can see better start on validation 34.764 PPL, no increasing loss and explosion loss on late train stages, but it learns very slowly and after 15 epochs on test shows only 21.465 PPL.\n",
        "\n",
        "## Model improvement ideas\n",
        "Results can be improved using\n",
        "1. bigger dataset (WMT for example)\n",
        "2. parameters difference in layers (as mentiones in paper: special parameters in first 10 layers and other in 3 last)\n",
        "3. data preprocessing - cleaning improvements/tests\n",
        "4. using SGD optimizer with Nesterov accelerated gradient. Yes, Adam shows better result. It is golden hammer, but can exist better result with Nesterov.\n",
        "5. using other models, Transformers for example:)\n",
        "6. keep optimizing hyperparameters (need more time and computing power)\n",
        "\n",
        "sergeev46v@gmail.com - additional information"
      ],
      "metadata": {
        "id": "dBYi6eEKAZbP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U torch==1.7.1\n",
        "!pip install -U torchtext==0.11.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hdqfl7wp9CN",
        "outputId": "a05c1f59-3754-4a8a-aa49-72d139286279"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchtext==0.11.0 in /usr/local/lib/python3.7/dist-packages (0.11.0)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.10.0+cu111)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (4.63.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (1.21.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.11.0) (2.23.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchtext==0.11.0) (3.10.0.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.11.0) (3.0.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "S3byd0BvAW58"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oaNw2Z9SnwCg"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.datasets import IWSLT2017\n",
        "from torchtext.legacy.data import Field, BucketIterator, TabularDataset\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ygDtj8Z5nwCh"
      },
      "outputs": [],
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en\n",
        "!python -m spacy download de"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6YOQpnsoXdB",
        "outputId": "6c276765-e592-49f8-a6e9-026071db4e44"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en_core_web_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz (12.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.0 MB 8.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n",
            "Collecting de_core_news_sm==2.2.5\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.2.5/de_core_news_sm-2.2.5.tar.gz (14.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 14.9 MB 10.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from de_core_news_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.6)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.0.6)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (4.63.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.9.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (57.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->de_core_news_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (4.11.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.7.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->de_core_news_sm==2.2.5) (2.10)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('de_core_news_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/de_core_news_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/de\n",
            "You can now load the model via spacy.load('de')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw01hq2OnwCi"
      },
      "source": [
        "Load the German and English spaCy models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "tFb_QhnAnwCj"
      },
      "outputs": [],
      "source": [
        "spacy_de = spacy.load('de')\n",
        "spacy_en = spacy.load('en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Rp0YMRnwCj"
      },
      "source": [
        "We create the tokenizers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "7jSYi6iXnwCk"
      },
      "outputs": [],
      "source": [
        "def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)]\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_en.tokenizer(text)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "_-DZD1DXnwCk"
      },
      "outputs": [],
      "source": [
        "SRC = Field(tokenize = tokenize_de, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)\n",
        "\n",
        "TRG = Field(tokenize = tokenize_en, \n",
        "            init_token = '<sos>', \n",
        "            eos_token = '<eos>', \n",
        "            lower = True, \n",
        "            batch_first = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YvQ8XsinwCl"
      },
      "source": [
        "Then, we load our dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nf5Ku8xEnwCl"
      },
      "outputs": [],
      "source": [
        "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), \n",
        "                                                    fields=(SRC, TRG))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EoD6k3ttnwCl"
      },
      "outputs": [],
      "source": [
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PwWcuBqHnwCm"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DgBuxZDyrzDn",
        "outputId": "237e7cc2-baba-4045-dc02-07f83e096449"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "mZVV-PBbnwCm"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "#Create an iterator below (using BucketIterator) to create your train, validation and test data.\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "     batch_size = BATCH_SIZE,\n",
        "     device = device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wd2wlJwenwCn"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 input_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        assert kernel_size % 2 == 1, \"Kernel size must be odd!\"\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size, \n",
        "                                              padding = (kernel_size - 1) // 2)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "               \n",
        "        batch_size = src.shape[0]\n",
        "        src_len = src.shape[1]\n",
        "        \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, src_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(src)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #pass embedded through linear layer to convert from emb dim to hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        #begin convolutional blocks...\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(self.dropout(conv_input))\n",
        "\n",
        "            #conved = [batch size, 2 * hid dim, src len]\n",
        "\n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "\n",
        "            #conved = [batch size, hid dim, src len]\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "        \n",
        "        #...end convolutional blocks\n",
        "        \n",
        "        #permute and convert back to emb dim\n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        #conved = [batch size, src len, emb dim]\n",
        "        \n",
        "        #elementwise sum output (conved) and input (embedded) to be used for attention\n",
        "        combined = (conved + embedded) * self.scale\n",
        "        \n",
        "        #combined = [batch size, src len, emb dim]\n",
        "        \n",
        "        return conved, combined"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oRQFVFyJnwCo"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, \n",
        "                 output_dim, \n",
        "                 emb_dim, \n",
        "                 hid_dim, \n",
        "                 n_layers, \n",
        "                 kernel_size, \n",
        "                 dropout, \n",
        "                 trg_pad_idx, \n",
        "                 device,\n",
        "                 max_length = 100):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.kernel_size = kernel_size\n",
        "        self.trg_pad_idx = trg_pad_idx\n",
        "        self.device = device\n",
        "        \n",
        "        self.scale = torch.sqrt(torch.FloatTensor([0.5])).to(device)\n",
        "        \n",
        "        self.tok_embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.pos_embedding = nn.Embedding(max_length, emb_dim)\n",
        "        \n",
        "        self.emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        self.hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        \n",
        "        self.attn_hid2emb = nn.Linear(hid_dim, emb_dim)\n",
        "        self.attn_emb2hid = nn.Linear(emb_dim, hid_dim)\n",
        "        \n",
        "        self.fc_out = nn.Linear(emb_dim, output_dim)\n",
        "        \n",
        "        self.convs = nn.ModuleList([nn.Conv1d(in_channels = hid_dim, \n",
        "                                              out_channels = 2 * hid_dim, \n",
        "                                              kernel_size = kernel_size)\n",
        "                                    for _ in range(n_layers)])\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "      \n",
        "    def calculate_attention(self, embedded, conved, encoder_conved, encoder_combined):\n",
        "                \n",
        "        #permute and convert back to emb dim\n",
        "        conved_emb = self.attn_hid2emb(conved.permute(0, 2, 1))\n",
        "        \n",
        "        combined = (conved_emb + embedded) * self.scale\n",
        "        \n",
        "        energy = torch.matmul(combined, encoder_conved.permute(0, 2, 1))\n",
        "        \n",
        "        attention = F.softmax(energy, dim=2)\n",
        "        \n",
        "        attended_encoding = torch.matmul(attention, encoder_combined)\n",
        "        \n",
        "        #convert from emb dim -> hid dim\n",
        "        attended_encoding = self.attn_emb2hid(attended_encoding)\n",
        "        \n",
        "        #apply residual connection\n",
        "        attended_combined = (conved + attended_encoding.permute(0, 2, 1)) * self.scale\n",
        "        \n",
        "        #attended_combined = [batch size, hid dim, trg len]\n",
        "        \n",
        "        return attention, attended_combined\n",
        "        \n",
        "    def forward(self, trg, encoder_conved, encoder_combined):\n",
        "        \n",
        "        batch_size = trg.shape[0]\n",
        "        trg_len = trg.shape[1]\n",
        "            \n",
        "        #create position tensor\n",
        "        pos = torch.arange(0, trg_len).unsqueeze(0).repeat(batch_size, 1).to(self.device)\n",
        "        \n",
        "        #embed tokens and positions\n",
        "        tok_embedded = self.tok_embedding(trg)\n",
        "        pos_embedded = self.pos_embedding(pos)\n",
        "        \n",
        "        #combine embeddings by elementwise summing\n",
        "        embedded = self.dropout(tok_embedded + pos_embedded)\n",
        "        \n",
        "        #pass embedded through linear layer to go through emb dim -> hid dim\n",
        "        conv_input = self.emb2hid(embedded)\n",
        "        \n",
        "        #permute for convolutional layer\n",
        "        conv_input = conv_input.permute(0, 2, 1) \n",
        "        \n",
        "        batch_size = conv_input.shape[0]\n",
        "        hid_dim = conv_input.shape[1]\n",
        "        \n",
        "        for i, conv in enumerate(self.convs):\n",
        "        \n",
        "            #apply dropout\n",
        "            conv_input = self.dropout(conv_input)\n",
        "        \n",
        "            #need to pad so decoder can't \"cheat\"\n",
        "            padding = torch.zeros(batch_size, \n",
        "                                  hid_dim, \n",
        "                                  self.kernel_size - 1).fill_(self.trg_pad_idx).to(self.device)\n",
        "                \n",
        "            padded_conv_input = torch.cat((padding, conv_input), dim = 2)\n",
        "        \n",
        "            #pass through convolutional layer\n",
        "            conved = conv(padded_conv_input)\n",
        "    \n",
        "            #pass through GLU activation function\n",
        "            conved = F.glu(conved, dim = 1)\n",
        "\n",
        "            #conved = [batch size, hid dim, trg len]\n",
        "            \n",
        "            #calculate attention\n",
        "            attention, conved = self.calculate_attention(embedded, \n",
        "                                                         conved, \n",
        "                                                         encoder_conved, \n",
        "                                                         encoder_combined)\n",
        "            \n",
        "            #attention = [batch size, trg len, src len]\n",
        "            \n",
        "            #apply residual connection\n",
        "            conved = (conved + conv_input) * self.scale\n",
        "            \n",
        "            #set conv_input to conved for next loop iteration\n",
        "            conv_input = conved\n",
        "            \n",
        "        conved = self.hid2emb(conved.permute(0, 2, 1))\n",
        "            \n",
        "        output = self.fc_out(self.dropout(conved))\n",
        "            \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uYYkFcaDnwCp"
      },
      "outputs": [],
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        \n",
        "    def forward(self, src, trg):\n",
        "        \n",
        "        \n",
        "        #calculate z^u (encoder_conved) and (z^u + e) (encoder_combined)\n",
        "        #encoder_conved is output from final encoder conv. block\n",
        "        #encoder_combined is encoder_conved plus (elementwise) src embedding plus \n",
        "        #  positional embeddings \n",
        "        encoder_conved, encoder_combined = self.encoder(src)\n",
        "        \n",
        "        #calculate predictions of next words\n",
        "        #output is a batch of predictions for each word in the trg sentence\n",
        "        #attention a batch of attention scores across the src sentence for \n",
        "        #  each word in the trg sentence\n",
        "        output, attention = self.decoder(trg, encoder_conved, encoder_combined)\n",
        "        \n",
        "        #output = [batch size, trg len - 1, output dim]\n",
        "        #attention = [batch size, trg len - 1, src len]\n",
        "        \n",
        "        return output, attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "ezRqzfJTnwCp"
      },
      "outputs": [],
      "source": [
        "INPUT_DIM = len(SRC.vocab)\n",
        "OUTPUT_DIM = len(TRG.vocab)\n",
        "EMB_DIM = 256\n",
        "HID_DIM = 512 # each conv. layer has 2 * hid_dim filters\n",
        "ENC_LAYERS = 15 # number of conv. blocks in encoder\n",
        "DEC_LAYERS = 15 # number of conv. blocks in decoder\n",
        "ENC_KERNEL_SIZE = 3 # must be odd!\n",
        "DEC_KERNEL_SIZE = 3 # can be even or odd\n",
        "ENC_DROPOUT = 0.25\n",
        "DEC_DROPOUT = 0.25\n",
        "TRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\n",
        "    \n",
        "enc = Encoder(INPUT_DIM, EMB_DIM, HID_DIM, ENC_LAYERS, ENC_KERNEL_SIZE, ENC_DROPOUT, device)\n",
        "dec = Decoder(OUTPUT_DIM, EMB_DIM, HID_DIM, DEC_LAYERS, DEC_KERNEL_SIZE, DEC_DROPOUT, TRG_PAD_IDX, device)\n",
        "\n",
        "model = Seq2Seq(enc, dec).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h6JZaspAnwCp",
        "outputId": "d739ab6a-cb0a-4854-bedd-97c19ed06dd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 53,090,565 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "fC66LlvxnwCp"
      },
      "outputs": [],
      "source": [
        "optimizer = optim.Adam(model.parameters())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#optimizer = torch.optim.SGD(model.parameters(), lr=0.25, momentum=0.99, nesterov=True)"
      ],
      "metadata": {
        "id": "fmnAkav6aW1J"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5mGDdso4nwCq"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "PFwRO5K9nwCq"
      },
      "outputs": [],
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output, _ = model(src, trg[:,:-1])\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output.contiguous().view(-1, output_dim)\n",
        "        trg = trg[:,1:].contiguous().view(-1)\n",
        "        \n",
        "        loss = criterion(output, trg)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "OIbCHRwXnwCq"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, _ = model(src, trg[:,:-1])\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output.contiguous().view(-1, output_dim)\n",
        "            trg = trg[:,1:].contiguous().view(-1)\n",
        "            \n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e0PA1J5OnwCq"
      },
      "outputs": [],
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "m9pgvTxxnwCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c03952ba-2b2d-4a1f-f1cf-23b788044447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Time: 1m 22s\n",
            "\tTrain Loss: 6.771 | Train PPL: 872.267\n",
            "\t Val. Loss: 4.837 |  Val. PPL: 126.091\n",
            "Epoch: 02 | Time: 1m 22s\n",
            "\tTrain Loss: 2.894 | Train PPL:  18.062\n",
            "\t Val. Loss: 2.554 |  Val. PPL:  12.862\n",
            "Epoch: 03 | Time: 1m 21s\n",
            "\tTrain Loss: 2.494 | Train PPL:  12.110\n",
            "\t Val. Loss: 2.274 |  Val. PPL:   9.715\n",
            "Epoch: 04 | Time: 1m 21s\n",
            "\tTrain Loss: 2.267 | Train PPL:   9.647\n",
            "\t Val. Loss: 2.083 |  Val. PPL:   8.032\n",
            "Epoch: 05 | Time: 1m 21s\n",
            "\tTrain Loss: 2.112 | Train PPL:   8.267\n",
            "\t Val. Loss: 1.949 |  Val. PPL:   7.023\n",
            "| Test Loss: 2.467 | Test PPL:  11.792 |\n",
            "Epoch: 06 | Time: 1m 21s\n",
            "\tTrain Loss: 1.994 | Train PPL:   7.347\n",
            "\t Val. Loss: 1.844 |  Val. PPL:   6.320\n",
            "Epoch: 07 | Time: 1m 21s\n",
            "\tTrain Loss: 1.899 | Train PPL:   6.677\n",
            "\t Val. Loss: 1.764 |  Val. PPL:   5.837\n",
            "Epoch: 08 | Time: 1m 21s\n",
            "\tTrain Loss: 1.826 | Train PPL:   6.207\n",
            "\t Val. Loss: 1.692 |  Val. PPL:   5.432\n",
            "Epoch: 09 | Time: 1m 21s\n",
            "\tTrain Loss: 1.760 | Train PPL:   5.810\n",
            "\t Val. Loss: 1.637 |  Val. PPL:   5.141\n",
            "Epoch: 10 | Time: 1m 21s\n",
            "\tTrain Loss: 1.704 | Train PPL:   5.495\n",
            "\t Val. Loss: 1.580 |  Val. PPL:   4.854\n",
            "| Test Loss: 2.370 | Test PPL:  10.698 |\n",
            "Epoch: 11 | Time: 1m 22s\n",
            "\tTrain Loss: 1.651 | Train PPL:   5.213\n",
            "\t Val. Loss: 1.524 |  Val. PPL:   4.593\n",
            "Epoch: 12 | Time: 1m 21s\n",
            "\tTrain Loss: 1.601 | Train PPL:   4.956\n",
            "\t Val. Loss: 1.496 |  Val. PPL:   4.462\n",
            "Epoch: 13 | Time: 1m 22s\n",
            "\tTrain Loss: 1.578 | Train PPL:   4.848\n",
            "\t Val. Loss: 1.684 |  Val. PPL:   5.385\n",
            "PPL explosion!\n"
          ]
        }
      ],
      "source": [
        "N_EPOCHS = 25\n",
        "CLIP = 0.1\n",
        "\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "#model.load_state_dict(torch.load('tut5-model.pt'))\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, train_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'tut5-model.pt')\n",
        "\n",
        "    if train_loss < 100000 and valid_loss < 10000:\n",
        "      print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "      print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "      print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "    else:\n",
        "      print('PPL explosion!')\n",
        "      break\n",
        "\n",
        "    if (epoch+1) % 5 == 0:\n",
        "      test_loss = evaluate(model, test_iterator, criterion)### The model should achieve a perplexity of 30 or lower on the test data.\n",
        "      if test_loss < 100000:\n",
        "        print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n",
        "      else:\n",
        "        break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "BBjj1gevnwCr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a35edd8-8bdd-4fc5-a720-cb1fc38ee8e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "| Test Loss: 2.296\n",
            "| Test Loss: 2.296 | Test PPL:   9.933 |\n"
          ]
        }
      ],
      "source": [
        "model.load_state_dict(torch.load('tut5-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)### The model should achieve a perplexity of 30 or lower on the test data.\n",
        "print(f'| Test Loss: {test_loss:.3f}')\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "5 - Convolutional Sequence to Sequence Learning.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}